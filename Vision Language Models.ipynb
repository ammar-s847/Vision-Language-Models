{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vision-Language Model Playground\n",
        "This notebook outlines experiments related to multi-modal vision and language models. Mainly  \n",
        "\n",
        "Models to test:\n",
        "- OpenAI CLIP\n",
        "- Facebook TimesFormer\n",
        "- VisualBert\n",
        "- Flamingo\n",
        "- UniLm\n",
        "- Large World Model\n"
      ],
      "metadata": {
        "id": "6piXkwO1EjaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeU9yWGyf_Bi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import requests\n",
        "import tqdm\n",
        "import pprint\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "sns.set_theme()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Overview\n",
        "- \"How many people are wearing red in this picture?\" <br>\n",
        "(image: https://live.staticflickr.com/3019/4554964811_a96d51b67b_b.jpg)\n"
      ],
      "metadata": {
        "id": "et2jsmtxEeew"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aktTnTeIEd5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI CLIP"
      ],
      "metadata": {
        "id": "zA19saF_nFzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
        "probs"
      ],
      "metadata": {
        "id": "mrG4bvj5gBlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d89ec53-f839-40a7-c949-38700260dae8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=[\"How many cats are in the image?\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "model(**inputs)"
      ],
      "metadata": {
        "id": "xtt9Y2EQgBo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Gj9Vt16gBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZFsCNnogBu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "se4BIGfLgBx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytQUvk0agB1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_YyZr23gCoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TimesFormer"
      ],
      "metadata": {
        "id": "YVXkT5B0nBsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
        "\n",
        "video = list(np.random.randn(8, 3, 224, 224))\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")\n",
        "model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")\n",
        "\n",
        "inputs = processor(images=video, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "  logits = outputs.logits\n",
        "\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "id": "06UE7Z1YgBV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAcv-Xt0gBiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VisualBERT"
      ],
      "metadata": {
        "id": "6iB0SzO1FckV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwbQMCL_Fkwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large World Model"
      ],
      "metadata": {
        "id": "rX3TbGaUFlG1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMBoB8OcFnPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MOO"
      ],
      "metadata": {
        "id": "Li4eha1qFnkq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SbIxKT0Fowv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EDJjyR0GFo8B"
      }
    }
  ]
}